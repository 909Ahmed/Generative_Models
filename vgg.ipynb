{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(image_path):\n",
    "\n",
    "    image = tf.image.decode_jpeg(tf.io.read_file(image_path))\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, [512, 512])\n",
    "    image = image[tf.newaxis, :]\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = get_images('./cry.jpg')\n",
    "style_image = get_images('./demon.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG19(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    ")\n",
    "vgg.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(content_layers, style_layers):\n",
    "    \n",
    "    content = [vgg.get_layer(layer).output for layer in content_layers]\n",
    "    style = [vgg.get_layer(layer).output for layer in style_layers]\n",
    "\n",
    "\n",
    "    model = Model([vgg.input], [content, style])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = ['block5_conv2']\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(content_layers, style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_style = model(preprocess_input(style_image * 255))[1]\n",
    "req_content = model(preprocess_input(content_image * 255))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e4\n",
    "beta = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(output, normalize=True):\n",
    "    \n",
    "    matrix = tf.linalg.einsum('nija,nijb->nab', output, output)\n",
    "    shape = tf.shape(matrix)\n",
    "    N = tf.cast(shape[1] * shape[2], tf.float32)\n",
    "    \n",
    "    if normalize:\n",
    "        matrix /= N\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(output):\n",
    "\n",
    "    content = output[0]\n",
    "    style = output[1]\n",
    "\n",
    "    style_loss = tf.add_n([tf.reduce_mean((gram_matrix(style[i]) - gram_matrix(req_style[i]))**2) for i in range(len(style_layers))]) / len(style_layers)\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content[i] - req_content[i])**2) for i in range(len(content_layers))]) / len(content_layers)\n",
    "\n",
    "    return alpha * content_loss + beta * style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "def clip(image):\n",
    "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        img = preprocess_input(image*255)\n",
    "        output = model(img)\n",
    "        loss = calc_loss(output)\n",
    "\n",
    "    grad = tape.gradient(loss, image)\n",
    "    optimizer.apply_gradient([grad, image])\n",
    "    image.assign(clip(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(epochs):\n",
    "    train(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "def tensor_to_image(tensor):\n",
    "  tensor = tensor*255\n",
    "  tensor = np.array(tensor, dtype=np.uint8)\n",
    "  if np.ndim(tensor)>3:\n",
    "    assert tensor.shape[0] == 1\n",
    "    tensor = tensor[0]\n",
    "  return PIL.Image.fromarray(tensor)\n",
    "\n",
    "tensor_to_image(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
